---
title: "Barbell Manner Prediction"
output: html_document
author: Yaxuan Li
date: "2023/8/10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.
-->

# Introduction

The goal of this project is to predict the manner in which they did the exercise. The **manner** that in the training set is the "classe" variable. So we will use the other variables to predict the manner. The process of this project is as follows:

## 1. Data Glance

First, we will load the data and have a glance of it.

```{r message=FALSE}
# load the training and testing data using readr
library(readr)
train <- read_csv("pml-training.csv", col_types = cols(), na = c("NA", "#DIV/0!", ""))
test <- read_csv("pml-testing.csv", col_types = cols(), na = c("NA", "#DIV/0!", ""))

```

```{r}
colnames(train)
# have a glance of the basic structure of the training data
sprintf("training data has %d rows and %d columns and testing data has %d rows and %d columns.", dim(train)[1], dim(train)[2], dim(test)[1], dim(test)[2])
```

As we can see, there are 19622 rows and 160 columns in the training data. The testing data has 20 rows and 160 columns. The first 7 columns are the username, timestamps, window number, and so on. The last 153 columns are the sensor data. The last column is the manner that we want to predict.

## 2. Data Cleaning

As what mentioned above, the first 7 columns are not useful for our prediction. So we will remove them.

```{r}
# remove the first 7 columns
train <- train[, 8:160]
test <- test[, 8:160]
```

Then we will remove the columns that have more than 90% missing values.

```{r}
# remove the columns that have more than 90% missing values
train <- train[, colSums(is.na(train)) < 0.9 * dim(train)[1]]
test <- test[, colSums(is.na(test)) < 0.9 * dim(test)[1]]
```

## 3. Data Partition

We will partition the training data into two parts. One is the training set and the other is the validation set. We will use the training set to build the model and use the validation set to evaluate the model.

```{r}
# partition the training data into two parts
library(caret)
set.seed(42)

inTrain <- caret::createDataPartition(y = train$classe, p = 0.70, list = FALSE)
training <- train[inTrain,]
validation <- train[-inTrain,]

training$classe <- factor(training$classe)
validation$classe <- factor(validation$classe)


```

## 4. Model Building

Since we have a large number of variables, random forest will be a good choice. We can use package **randomForest** to build the model.

```{r}
# build the model using random forest
library(randomForest)
model <- randomForest(factor(training$classe) ~ ., data = training)
```

## 5. Model Evaluation

We will use the validation set to evaluate the model and see the accuracy of the model. Also, we will use the confusion matrix to see the prediction result.

```{r}
# evaluate the model using the validation set
pred <- predict(model, validation)
caret::confusionMatrix(pred, validation$classe)

```

As we can see, the accuracy of the model is 0.995. The confusion matrix shows that the model has a good prediction result.

## 6. Prediction

Finally, we will use the model to predict the manner of the testing data.

```{r}
# predict the manner of the testing data
pred_test <- predict(model, test)
pred_test
```
